{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Getting 12963 valid examples from training set.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "class Webis17:\n",
    "    truth_file = None\n",
    "    problem_file = None\n",
    "    corpus = [] # (title, paragraphs, label)\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.truth_file = path + 'truth.jsonl'\n",
    "        self.problem_file = path + 'instances.jsonl'\n",
    "\n",
    "    def get_truths(self, size=100):\n",
    "        df = pd.read_json(self.truth_file, lines=True)\n",
    "        df = df.loc[:size, :]\n",
    "        return df['id'], df['truthMean'].values\n",
    "\n",
    "    def get_texts(self, size=100):\n",
    "        df = pd.read_json(self.problem_file, lines=True)\n",
    "        df = df.loc[:size, :]\n",
    "        return df['id'], df['targetTitle'], df['targetParagraphs']\n",
    "\n",
    "    def build_corpus(self, size=100):\n",
    "        (truth_id, label) = self.get_truths(size)\n",
    "        ground_truth = {truth_id[i] : label[i] for i in range(len(label))}\n",
    "        (tweet_id, titles, texts) = self.get_texts(size)\n",
    "        for i, tid in enumerate(tweet_id):\n",
    "            try:\n",
    "                if abs(ground_truth[tid] - 0.5) > 0.2: # getting only high confidence examples\n",
    "                    self.corpus.append( (titles[i], ' '.join(txt for txt in texts[i]), ground_truth[tid]) ) # tid is discarded from now on\n",
    "            except KeyError:\n",
    "                print(f'Tweet {tid} is not in ground truth!')\n",
    "                pass\n",
    "        print(f'Getting {len(self.corpus)} valid examples from training set.')\n",
    "web17 = Webis17('./data/clickbait17/')\n",
    "web17.build_corpus(size=19538)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "sentences = []\n",
    "truths = []\n",
    "for tweet in web17.corpus:\n",
    "    title, paragraph, label = tweet\n",
    "    sent_pairs = []\n",
    "    # for each sent in paragraph, pair it with the text\n",
    "    for sent in nltk.sent_tokenize(paragraph):\n",
    "        #sent_pairs.append(\"[CLS] \" + title + \" [SEP]\" + sent + \"[sep]\")\n",
    "        sent_pairs.append( (title, sent) )\n",
    "    sentences.append(sent_pairs)\n",
    "    truths.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentences, truths, test_size=0.01)\n",
    "class Trainset():\n",
    "    def __getitem__(self, i):\n",
    "        return X_train[i], y_train[i]\n",
    "    def __len__(self):\n",
    "        return len(y_train)\n",
    "class Testset():\n",
    "    def __getitem__(self, i):\n",
    "        return X_test[i], y_test[i]\n",
    "    def __len__(self):\n",
    "        return len(y_test)\n",
    "trainset, testset = Trainset(), Testset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attentions(outputs, layer=0, attention_head=0, avg=False):\n",
    "    '''\n",
    "    get the particular output for a particular layer and attention head\n",
    "    layer -> 0 to 11\n",
    "    attention_head -> 0 to 11\n",
    "    '''\n",
    "    if avg:\n",
    "        #avg over all attention heads in a layer\n",
    "        return outputs[layer].squeeze(0).mean(dim=0)  \n",
    "    #return values for a particular attention head inside a specific layer\n",
    "    return outputs[layer].squeeze(0)[attention_head]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True,)\n",
    "bert_model.eval()\n",
    "\n",
    "COS = torch.nn.CosineSimilarity(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (29) must match the size of tensor b (114) at non-singleton dimension 0",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-92a1caf3ece9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mattention1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_attentions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mattention2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_attentions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0msimilarity_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mCOS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/distance.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (29) must match the size of tensor b (114) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = torch.utils.data.DataLoader( trainset, batch_size, shuffle=True )\n",
    "test_loader = torch.utils.data.DataLoader( testset, batch_size, shuffle=True )\n",
    "\n",
    "preds = []\n",
    "labels = []\n",
    "\n",
    "for data, label in test_loader:\n",
    "    similarity_scores = []\n",
    "    for sent_pair in data:\n",
    "        (title, sent) = sent_pair\n",
    "        input1 = tokenizer(title, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        input2 = tokenizer(sent,  return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "        #print(input1)\n",
    "        #print(input2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs1 = bert_model(**input1)\n",
    "            outputs2 = bert_model(**input2)\n",
    "            attention1 = get_attentions(outputs1).detach()\n",
    "            attention2 = get_attentions(outputs2).detach()\n",
    "            similarity_scores.append( COS(attention1, attention2).item() )\n",
    "    score = np.mean(np.array(similarity_scores))\n",
    "    preds.append(score)\n",
    "    labels.append(label)"
   ]
  }
 ]
}